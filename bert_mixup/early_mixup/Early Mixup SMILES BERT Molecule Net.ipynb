{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a5d9e095",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/23/2022 13:48:49 - INFO - deepchem.molnet.load_function.molnet_loader -   About to featurize bace_c dataset.\n",
      "12/23/2022 13:48:49 - INFO - deepchem.data.data_loader -   Loading raw samples now.\n",
      "12/23/2022 13:48:49 - INFO - deepchem.data.data_loader -   shard_size: 8192\n",
      "12/23/2022 13:48:49 - INFO - deepchem.utils.data_utils -   About to start loading CSV from /var/folders/s4/4l6cbdrn7cq2m4vs14xz9qpm0000gn/T/bace.csv\n",
      "12/23/2022 13:48:49 - INFO - deepchem.utils.data_utils -   Loading shard 1 of size 8192.\n",
      "12/23/2022 13:48:49 - INFO - deepchem.data.data_loader -   About to featurize shard.\n",
      "12/23/2022 13:48:49 - INFO - deepchem.feat.base_classes -   Featurizing datapoint 0\n",
      "12/23/2022 13:48:55 - INFO - deepchem.feat.base_classes -   Featurizing datapoint 1000\n",
      "12/23/2022 13:48:58 - INFO - deepchem.data.data_loader -   TIMING: featurizing shard 0 took 9.081 s\n",
      "12/23/2022 13:48:59 - INFO - deepchem.data.datasets -   TIMING: dataset construction took 9.270 s\n",
      "12/23/2022 13:48:59 - INFO - deepchem.data.datasets -   Loading dataset from disk.\n",
      "12/23/2022 13:48:59 - INFO - deepchem.molnet.load_function.molnet_loader -   About to split dataset with ScaffoldSplitter splitter.\n",
      "12/23/2022 13:48:59 - INFO - deepchem.splits.splitters -   Computing train/valid/test indices\n",
      "12/23/2022 13:48:59 - INFO - deepchem.splits.splitters -   About to generate scaffolds\n",
      "12/23/2022 13:48:59 - INFO - deepchem.splits.splitters -   Generating scaffold 0/1513\n",
      "12/23/2022 13:48:59 - INFO - deepchem.splits.splitters -   Generating scaffold 1000/1513\n",
      "12/23/2022 13:48:59 - INFO - deepchem.splits.splitters -   About to sort in scaffold sets\n",
      "12/23/2022 13:48:59 - INFO - deepchem.data.datasets -   Constructing selection output shard 1\n",
      "12/23/2022 13:48:59 - INFO - deepchem.data.datasets -   Selecting from input shard 1/1 for selection output shard 1\n",
      "12/23/2022 13:48:59 - INFO - deepchem.data.datasets -   TIMING: dataset construction took 0.072 s\n",
      "12/23/2022 13:48:59 - INFO - deepchem.data.datasets -   Loading dataset from disk.\n",
      "12/23/2022 13:48:59 - INFO - deepchem.data.datasets -   Constructing selection output shard 1\n",
      "12/23/2022 13:48:59 - INFO - deepchem.data.datasets -   Selecting from input shard 1/1 for selection output shard 1\n",
      "12/23/2022 13:48:59 - INFO - deepchem.data.datasets -   TIMING: dataset construction took 0.020 s\n",
      "12/23/2022 13:48:59 - INFO - deepchem.data.datasets -   Loading dataset from disk.\n",
      "12/23/2022 13:48:59 - INFO - deepchem.data.datasets -   Constructing selection output shard 1\n",
      "12/23/2022 13:48:59 - INFO - deepchem.data.datasets -   Selecting from input shard 1/1 for selection output shard 1\n",
      "12/23/2022 13:49:00 - INFO - deepchem.data.datasets -   TIMING: dataset construction took 0.031 s\n",
      "12/23/2022 13:49:00 - INFO - deepchem.data.datasets -   Loading dataset from disk.\n",
      "12/23/2022 13:49:00 - INFO - deepchem.molnet.load_function.molnet_loader -   About to transform data.\n",
      "12/23/2022 13:49:00 - INFO - deepchem.data.datasets -   Transforming shard 0/1\n",
      "12/23/2022 13:49:00 - INFO - deepchem.data.datasets -   TIMING: dataset construction took 0.058 s\n",
      "12/23/2022 13:49:00 - INFO - deepchem.data.datasets -   Loading dataset from disk.\n",
      "12/23/2022 13:49:00 - INFO - deepchem.data.datasets -   TIMING: transforming took 0.063 s\n",
      "12/23/2022 13:49:00 - INFO - deepchem.data.datasets -   Transforming shard 0/1\n",
      "12/23/2022 13:49:00 - INFO - deepchem.data.datasets -   TIMING: dataset construction took 0.016 s\n",
      "12/23/2022 13:49:00 - INFO - deepchem.data.datasets -   Loading dataset from disk.\n",
      "12/23/2022 13:49:00 - INFO - deepchem.data.datasets -   TIMING: transforming took 0.023 s\n",
      "12/23/2022 13:49:00 - INFO - deepchem.data.datasets -   Transforming shard 0/1\n",
      "12/23/2022 13:49:00 - INFO - deepchem.data.datasets -   TIMING: dataset construction took 0.021 s\n",
      "12/23/2022 13:49:00 - INFO - deepchem.data.datasets -   Loading dataset from disk.\n",
      "12/23/2022 13:49:00 - INFO - deepchem.data.datasets -   TIMING: transforming took 0.035 s\n"
     ]
    }
   ],
   "source": [
    "from deepchem.molnet import load_bace_classification, load_bbbp\n",
    "import numpy as np\n",
    "\n",
    "from simcse import SimCSE\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "from args_parser import parse_args\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "datasets = {\n",
    "        \"bace\": load_bace_classification,\n",
    "        \"bbbp\": load_bbbp\n",
    "        }\n",
    "\n",
    "sys.argv = ['']\n",
    "args = parse_args()\n",
    "\n",
    "args.samples_per_class=100\n",
    "args.n_augment = 2\n",
    "args.batch_size = 8\n",
    "\n",
    "input_dim = 512\n",
    "output_dim = args.num_labels\n",
    "set_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "6a2cf6f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at shahrukhx01/muv2x-simcse-smole-bert were not used when initializing BertModel: ['mlp.dense.bias', 'mlp.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at shahrukhx01/muv2x-simcse-smole-bert and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 4/4 [00:08<00:00,  2.13s/it]\n",
      "100%|██████████| 3/3 [00:05<00:00,  1.80s/it]\n",
      "100%|██████████| 3/3 [00:09<00:00,  3.14s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = SimCSE(\"shahrukhx01/muv2x-simcse-smole-bert\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "04739204",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'train_idx = [_idx for _idx in range(len(train_embeddings))]\\naugmented_embeds, augmented_labels, lamdas = [], [], []\\nif args.n_augment:\\n    for indices_batch in batch(train_idx, args.batch_size):\\n        print(len(indices_batch))\\n        for i in range(args.n_augment):\\n            np.random.seed()\\n            lam = np.random.beta(args.alpha, args.alpha)\\n            embedding2_idx = np.random.choice(train_embeddings_idx, replace=False)\\n            embedding2 = train_embeddings[embedding2_idx, :]\\n            label2 = train_labels[embedding2_idx]\\n            aug_embed, aug_label =  mixup_augment(embedding1=train_embedding, embedding2=embedding2, label1=train_label+1, label2=label2+1, lamda=lam)\\n            aug_label = aug_label-1\\n            augmented_embeds.append(aug_embed)\\n            augmented_labels.append(aug_label)\\n\\ntrain_embeddings_augmented, train_labels_augmented = None, None\\nif len(augmented_embeds):\\n    augmented_embeds = torch.stack(augmented_embeds)\\n    train_embeddings_augmented = torch.cat([train_embeddings, augmented_embeds])\\n    train_labels_augmented = train_labels + augmented_labels\\nelse:\\n    train_embeddings_augmented, train_labels_augmented = train_embeddings, train_labels\\ntrain_embeddings_augmented.shape'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"train_idx = [_idx for _idx in range(len(train_embeddings))]\n",
    "augmented_embeds, augmented_labels, lamdas = [], [], []\n",
    "if args.n_augment:\n",
    "    for indices_batch in batch(train_idx, args.batch_size):\n",
    "        print(len(indices_batch))\n",
    "        for i in range(args.n_augment):\n",
    "            np.random.seed()\n",
    "            lam = np.random.beta(args.alpha, args.alpha)\n",
    "            embedding2_idx = np.random.choice(train_embeddings_idx, replace=False)\n",
    "            embedding2 = train_embeddings[embedding2_idx, :]\n",
    "            label2 = train_labels[embedding2_idx]\n",
    "            aug_embed, aug_label =  mixup_augment(embedding1=train_embedding, embedding2=embedding2, label1=train_label+1, label2=label2+1, lamda=lam)\n",
    "            aug_label = aug_label-1\n",
    "            augmented_embeds.append(aug_embed)\n",
    "            augmented_labels.append(aug_label)\n",
    "\n",
    "train_embeddings_augmented, train_labels_augmented = None, None\n",
    "if len(augmented_embeds):\n",
    "    augmented_embeds = torch.stack(augmented_embeds)\n",
    "    train_embeddings_augmented = torch.cat([train_embeddings, augmented_embeds])\n",
    "    train_labels_augmented = train_labels + augmented_labels\n",
    "else:\n",
    "    train_embeddings_augmented, train_labels_augmented = train_embeddings, train_labels\n",
    "train_embeddings_augmented.shape\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "43be7c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class MolNet(nn.Module):\n",
    "    \"\"\"\n",
    "    This class is created to specify the Neural Network on which vectorized datasets we have created previously\n",
    "    is trained on, validated and later tested.\n",
    "    It consist of one input layer, one output layer and multiple hidden layers.\n",
    "    ...\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, output_dim, dropout=0.5):\n",
    "        super(MolNet, self).__init__()\n",
    "        # Layer definitions\n",
    "        self.layers = nn.Sequential(\n",
    "        nn.Linear(input_dim, 1024),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(dropout),\n",
    "        nn.Linear(1024, 512),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(dropout),\n",
    "        nn.Linear(512, 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(dropout),\n",
    "        nn.Linear(256, 128),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(dropout),\n",
    "        nn.Linear(128, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass\n",
    "        return self.layers(x)\n",
    "\n",
    "model_mlp = MolNet(input_dim=input_dim, output_dim=output_dim).to(set_device)\n",
    "criterion = nn.CrossEntropyLoss().to(set_device)\n",
    "optimizer = getattr(optim, \"Adam\")(model_mlp.parameters(), lr=args.lr)\n",
    "\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from torch import sigmoid\n",
    "from torch.nn.functional import softmax\n",
    "\n",
    "\n",
    "def flat_auroc_score(preds, labels):\n",
    "    \"\"\"\n",
    "    Function to calculate the roc_auc_score of our predictions vs labels\n",
    "    \"\"\"\n",
    "    pred_flat = softmax(preds, dim=1)[:, 1]\n",
    "    # labels_flat = np.argmax(labels, axis=1)\n",
    "    return roc_auc_score(labels, pred_flat.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e707370f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_device.type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a8944eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixup_augment(embedding1, embedding2, label1, label2, lamda):\n",
    "    embedding_output = lam * embedding1 + (1.0 - lam) * embedding2\n",
    "    label_output = lam * label1 + (1.0 - lam) * label2\n",
    "    return (embedding_output, label_output)\n",
    "\n",
    "def get_perm(x):\n",
    "        \"\"\"get random permutation\"\"\"\n",
    "        batch_size = x.size()[0]\n",
    "        if args.cuda and torch.cuda.is_available():\n",
    "            index = torch.randperm(batch_size).cuda()\n",
    "        else:\n",
    "            index = torch.randperm(batch_size)\n",
    "        return index\n",
    "def mixup_criterion_cross_entropy(pred, y_a, y_b, lam):\n",
    "        return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "833e3e8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training =>  Epoch : 1 | Loss : 0.698230789899826\n",
      "Validation =>  Epoch : 1 | Loss : 0.6938460469245911 | AUROC score: 0.619047619047619 \n",
      "Selecting the model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-79-6be130c99b77>:65: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  targets = torch.LongTensor([x.item() for x in list(targets)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training =>  Epoch : 2 | Loss : 0.6901604342460632\n",
      "Validation =>  Epoch : 2 | Loss : 0.6785762310028076 | AUROC score: 0.6293532338308457 \n",
      "Selecting the model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-79-6be130c99b77>:65: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  targets = torch.LongTensor([x.item() for x in list(targets)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training =>  Epoch : 3 | Loss : 0.6690832436084747\n",
      "Validation =>  Epoch : 3 | Loss : 0.7490429282188416 | AUROC score: 0.6663113006396588 \n",
      "Selecting the model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-79-6be130c99b77>:65: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  targets = torch.LongTensor([x.item() for x in list(targets)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training =>  Epoch : 4 | Loss : 0.6269194722175598\n",
      "Validation =>  Epoch : 4 | Loss : 0.7419724464416504 | AUROC score: 0.6897654584221748 \n",
      "Selecting the model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-79-6be130c99b77>:65: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  targets = torch.LongTensor([x.item() for x in list(targets)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training =>  Epoch : 5 | Loss : 0.6441965830326081\n",
      "Validation =>  Epoch : 5 | Loss : 0.661343514919281 | AUROC score: 0.6769722814498934 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-79-6be130c99b77>:65: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  targets = torch.LongTensor([x.item() for x in list(targets)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training =>  Epoch : 6 | Loss : 0.6094879585504532\n",
      "Validation =>  Epoch : 6 | Loss : 0.6267139911651611 | AUROC score: 0.7306325515280739 \n",
      "Selecting the model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-79-6be130c99b77>:65: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  targets = torch.LongTensor([x.item() for x in list(targets)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training =>  Epoch : 7 | Loss : 0.601060526072979\n",
      "Validation =>  Epoch : 7 | Loss : 0.6589340567588806 | AUROC score: 0.7183724235963042 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-79-6be130c99b77>:65: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  targets = torch.LongTensor([x.item() for x in list(targets)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training =>  Epoch : 8 | Loss : 0.6007289463281631\n",
      "Validation =>  Epoch : 8 | Loss : 0.6080448627471924 | AUROC score: 0.7267235252309879 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-79-6be130c99b77>:65: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  targets = torch.LongTensor([x.item() for x in list(targets)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training =>  Epoch : 9 | Loss : 0.559790580868721\n",
      "Validation =>  Epoch : 9 | Loss : 0.7345520853996277 | AUROC score: 0.7006041222459133 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-79-6be130c99b77>:65: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  targets = torch.LongTensor([x.item() for x in list(targets)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training =>  Epoch : 10 | Loss : 0.5442842531204224\n",
      "Validation =>  Epoch : 10 | Loss : 0.6777929067611694 | AUROC score: 0.7006041222459132 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-79-6be130c99b77>:65: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  targets = torch.LongTensor([x.item() for x in list(targets)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training =>  Epoch : 11 | Loss : 0.5545399427413941\n",
      "Validation =>  Epoch : 11 | Loss : 0.787473201751709 | AUROC score: 0.705223880597015 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-79-6be130c99b77>:65: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  targets = torch.LongTensor([x.item() for x in list(targets)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training =>  Epoch : 12 | Loss : 0.5462082731723785\n",
      "Validation =>  Epoch : 12 | Loss : 0.6460936665534973 | AUROC score: 0.7311656005685856 \n",
      "Selecting the model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-79-6be130c99b77>:65: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  targets = torch.LongTensor([x.item() for x in list(targets)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training =>  Epoch : 13 | Loss : 0.5588362371921539\n",
      "Validation =>  Epoch : 13 | Loss : 0.6722927689552307 | AUROC score: 0.7080668088130775 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-79-6be130c99b77>:65: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  targets = torch.LongTensor([x.item() for x in list(targets)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training =>  Epoch : 14 | Loss : 0.4981475359201431\n",
      "Validation =>  Epoch : 14 | Loss : 0.7137606739997864 | AUROC score: 0.709132906894101 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-79-6be130c99b77>:65: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  targets = torch.LongTensor([x.item() for x in list(targets)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training =>  Epoch : 15 | Loss : 0.5341985869407654\n",
      "Validation =>  Epoch : 15 | Loss : 0.6894780397415161 | AUROC score: 0.7061122956645345 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-79-6be130c99b77>:65: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  targets = torch.LongTensor([x.item() for x in list(targets)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training =>  Epoch : 16 | Loss : 0.5234269896149635\n",
      "Validation =>  Epoch : 16 | Loss : 0.7153192162513733 | AUROC score: 0.7205046197583511 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-79-6be130c99b77>:65: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  targets = torch.LongTensor([x.item() for x in list(targets)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training =>  Epoch : 17 | Loss : 0.4725951021909714\n",
      "Validation =>  Epoch : 17 | Loss : 0.7400338649749756 | AUROC score: 0.7213930348258706 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-79-6be130c99b77>:65: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  targets = torch.LongTensor([x.item() for x in list(targets)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training =>  Epoch : 18 | Loss : 0.4876276111602783\n",
      "Validation =>  Epoch : 18 | Loss : 0.6653982400894165 | AUROC score: 0.7380952380952381 \n",
      "Selecting the model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-79-6be130c99b77>:65: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  targets = torch.LongTensor([x.item() for x in list(targets)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training =>  Epoch : 19 | Loss : 0.4824518275260925\n",
      "Validation =>  Epoch : 19 | Loss : 0.7567782402038574 | AUROC score: 0.7121535181236673 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-79-6be130c99b77>:65: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  targets = torch.LongTensor([x.item() for x in list(targets)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training =>  Epoch : 20 | Loss : 0.4674354638159275\n",
      "Validation =>  Epoch : 20 | Loss : 0.6789330244064331 | AUROC score: 0.7032693674484719 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-79-6be130c99b77>:65: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  targets = torch.LongTensor([x.item() for x in list(targets)])\n"
     ]
    }
   ],
   "source": [
    "best_model = None\n",
    "best_accuracy = 0.0\n",
    "train_loss_history, recall_train_history = [], []\n",
    "validation_loss_history, recall_validation_history = list(), list()\n",
    "for epoch in range(0, args.epoch):\n",
    "        model_mlp.train()\n",
    "        train_loss_scores = []\n",
    "        training_acc_scores = []\n",
    "        y_pred, y_true= list(), list()\n",
    "        predictions = []\n",
    "        for x, y in train_dataloader:\n",
    "            ## perform forward pass  \n",
    "            x = x.type(torch.FloatTensor).to(set_device)\n",
    "            y = y.type(torch.LongTensor).to(set_device)\n",
    "            for i in range(args.n_augment):\n",
    "                lam = np.random.beta(args.alpha, args.alpha)\n",
    "                indices_permuted = get_perm(x)\n",
    "                x2 = x[indices_permuted, :]\n",
    "                y2 = y[indices_permuted]\n",
    "                mixup_x, mixup_y = mixup_augment(embedding1=x, embedding2=x2, label1=y, label2=y2, lamda=lam)\n",
    "            \n",
    "                pred = model_mlp(mixup_x) \n",
    "\n",
    "                preds = torch.max(pred, 1)[1]\n",
    "\n",
    "                ## accumulate predictions per batch for the epoch\n",
    "                \"\"\"y_pred += list([x.item() for x in preds.detach().cpu().numpy()])\n",
    "                targets = torch.LongTensor([x.item() for x in list(targets)])\n",
    "                y_true +=  list([x.item() for x in targets.detach().cpu().numpy()])\"\"\"\n",
    "\n",
    "                ## compute loss and perform backward pass\n",
    "                loss = mixup_criterion_cross_entropy(pred, y, y2, lam=lam) ## compute loss \n",
    "                optimizer.zero_grad()\n",
    "                loss.backward() \n",
    "                optimizer.step()\n",
    "\n",
    "                predictions.append(pred)\n",
    "\n",
    "                ## accumulate train loss\n",
    "                train_loss_scores.append(loss.item())\n",
    "\n",
    "        ## accumulate loss, recall, f1, precision per epoch\n",
    "        train_loss_history.append((sum(train_loss_scores)/len(train_loss_scores)))\n",
    "        #recall = flat_auroc_score(predictions, y_true)\n",
    "        #recall_train_history.append(recall)\n",
    "        print(f'Training =>  Epoch : {epoch+1} | Loss : {train_loss_history[-1]}') \n",
    "          #| AUROC score: {recall_train_history[-1]}')\n",
    "        \n",
    "        model_mlp.eval()\n",
    "        predictions = None\n",
    "        with torch.no_grad():\n",
    "            validation_loss_scores = list()\n",
    "            y_true_val, y_pred_val= list(), list()\n",
    "\n",
    "            ## perform validation pass\n",
    "            for batch, targets in val_dataloader:\n",
    "                ## perform forward pass  \n",
    "                batch = batch.type(torch.FloatTensor).to(set_device)\n",
    "                pred = model_mlp(batch) \n",
    "                predictions = pred\n",
    "                preds = torch.max(pred, 1)[1]\n",
    "                \n",
    "                ## accumulate predictions per batch for the epoch\n",
    "                y_pred_val += list([x.item() for x in preds.detach().cpu().numpy()])\n",
    "                targets = torch.LongTensor([x.item() for x in list(targets)])\n",
    "                y_true_val +=  list([x.item() for x in targets.detach().cpu().numpy()])\n",
    "                \n",
    "                ## computing validate loss\n",
    "                loss = criterion(pred.to(set_device), targets.to(set_device)) ## compute loss \n",
    "\n",
    "                ## accumulate validate loss\n",
    "                validation_loss_scores.append(loss.item())\n",
    "                \n",
    "            \n",
    "            ## accumulate loss, recall, f1, precision per epoch\n",
    "            validation_loss_history.append((sum(validation_loss_scores)/len(validation_loss_scores)))\n",
    "            recall = flat_auroc_score(predictions, y_true_val)\n",
    "            recall_validation_history.append(recall)\n",
    "\n",
    "            print(f'Validation =>  Epoch : {epoch+1} | Loss : {validation_loss_history[-1]} | AUROC score: {recall_validation_history[-1]} ')\n",
    "            \n",
    "            if recall_validation_history[-1]>best_accuracy:\n",
    "                best_accuracy = recall_validation_history[-1]\n",
    "                print('Selecting the model...')\n",
    "                best_model = model_mlp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98dda79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "8edcf9dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test => AUROC score: 0.6110507246376812 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-80-f9938ec2c9f4>:17: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  targets = torch.LongTensor([x.item() for x in list(targets)])\n"
     ]
    }
   ],
   "source": [
    "best_model.eval()\n",
    "predictions = None\n",
    "with torch.no_grad():\n",
    "    validation_loss_scores = list()\n",
    "    y_true_val, y_pred_val= list(), list()\n",
    "\n",
    "    ## perform validation pass\n",
    "    for batch, targets in test_dataloader:\n",
    "        ## perform forward pass  \n",
    "        batch = batch.type(torch.FloatTensor).to(set_device)\n",
    "        pred = best_model(batch) \n",
    "        predictions = pred\n",
    "        preds = torch.max(pred, 1)[1]\n",
    "\n",
    "        ## accumulate predictions per batch for the epoch\n",
    "        y_pred_val += list([x.item() for x in preds.detach().cpu().numpy()])\n",
    "        targets = torch.LongTensor([x.item() for x in list(targets)])\n",
    "        y_true_val +=  list([x.item() for x in targets.detach().cpu().numpy()])\n",
    "\n",
    "        ## computing validate loss\n",
    "        loss = criterion(pred.to(set_device), targets.to(set_device)) ## compute loss \n",
    "\n",
    "        ## accumulate validate loss\n",
    "        validation_loss_scores.append(loss.item())\n",
    "\n",
    "\n",
    "    ## accumulate loss, recall, f1, precision per epoch\n",
    "    validation_loss_history.append((sum(validation_loss_scores)/len(validation_loss_scores)))\n",
    "    recall = flat_auroc_score(predictions, y_true_val)\n",
    "    recall_validation_history.append(recall)\n",
    "\n",
    "    print(f'Test => AUROC score: {recall_validation_history[-1]} ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54334bbd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
